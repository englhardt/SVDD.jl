<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>SVDD · SVDD Documentation</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><script src="../../copy.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">SVDD Documentation</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../start/">Getting Started</a></li><li class="is-active"><a class="tocitem" href>SVDD</a><ul class="internal"><li><a class="tocitem" href="#SVDD-Overview"><span>SVDD Overview</span></a></li><li><a class="tocitem" href="#Derivation-of-the-Dual-Problem"><span>Derivation of the Dual Problem</span></a></li><li class="toplevel"><a class="tocitem" href="#SVDDneg"><span>SVDDneg</span></a></li></ul></li><li><span class="tocitem">Custom Solvers</span><ul><li><a class="tocitem" href="../smo/">SMO</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>SVDD</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>SVDD</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/englhardt/SVDD.jl/blob/master/docs/src/svdd.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="svdd_doc"><a class="docs-heading-anchor" href="#svdd_doc">SVDD</a><a id="svdd_doc-1"></a><a class="docs-heading-anchor-permalink" href="#svdd_doc" title="Permalink"></a></h1><p>Support Vector Data Description (SVDD) is a one-class classifier and was first presented in [1].</p><p>This documentation first presents the optimization problem and then guides the guides the reader to the dual problem</p><h2 id="SVDD-Overview"><a class="docs-heading-anchor" href="#SVDD-Overview">SVDD Overview</a><a id="SVDD-Overview-1"></a><a class="docs-heading-anchor-permalink" href="#SVDD-Overview" title="Permalink"></a></h2><p>SVDD is an optimization problem of the following form.</p><p class="math-container">\[  \begin{aligned}
  P: \ &amp; \underset{R, a, \xi}{\text{min}}
  &amp; &amp; R^2 + C * \sum_i \xi_i  \\
  &amp; \text{s.t.}
  &amp; &amp; \left\Vert \Phi(x_{i}) - a \right\Vert^2 \leq R^2 + \xi_i, \; ∀ i \\
  &amp; &amp; &amp; \xi_i \geq 0, \; ∀ i
  \end{aligned}\]</p><p>with radius <span>$R$</span> and center of the hypersphere <span>$a$</span>, a slack variable <span>$\xi$</span>, and a mapping into an implicit feature space <span>$\Phi$</span>.</p><p>The dual Problem of the Lagrangian is:</p><p class="math-container">\[\begin{aligned}
D: \ &amp; \underset{\alpha}{\text{max}}
&amp; &amp; \sum_{i}\alpha_i K_{i,i} - \sum_i\sum_j\alpha_i\alpha_jK_{i,j}  \\
&amp; \text{s.t.}
&amp; &amp; \sum_i \alpha_i = 1 \\
&amp; &amp; &amp; 0 \leq \alpha_i \leq C, \; ∀ i \\
\end{aligned}\]</p><p>where <span>$\alpha$</span> are the Lagrange multipliers, and <span>$K_{i,j} = \langle {\Phi(x_i),\Phi{x_j}} \rangle$</span> the inner product in the implicit feature space. Solving the Lagrangian gives an optimal <span>$α$</span>. The following rules are valid for the optimal <span>$α$</span>:</p><p class="math-container">\[\begin{aligned}
\text{(I)} \quad &amp; \left\Vert \Phi(x_{i}) - a \right\Vert^2 \lt R^2 &amp; ⇒ &amp;\quad α_i = 0\\
\text{(II)} \quad &amp; \left\Vert \Phi(x_{i}) - a \right\Vert^2 = R^2 &amp; ⇒ &amp;\quad 0 &lt; α_i &lt; C\\
\text{(III)} \quad &amp; \left\Vert \Phi(x_{i}) - a \right\Vert^2 \gt R^2 &amp; ⇒ &amp;\quad α_i = C
\end{aligned}\]</p><p>An observation is an inlier if (I) or (II) holds and an outlier if (III) holds. Additionally, an observation is called a support vector (SV) if (II) holds. One can calculate the radius of the hypersphere from any SV as we will explain later.</p><h2 id="Derivation-of-the-Dual-Problem"><a class="docs-heading-anchor" href="#Derivation-of-the-Dual-Problem">Derivation of the Dual Problem</a><a id="Derivation-of-the-Dual-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#Derivation-of-the-Dual-Problem" title="Permalink"></a></h2><h4 id="Preliminaries-on-Lagrangian-duals."><a class="docs-heading-anchor" href="#Preliminaries-on-Lagrangian-duals.">Preliminaries on Lagrangian duals.</a><a id="Preliminaries-on-Lagrangian-duals.-1"></a><a class="docs-heading-anchor-permalink" href="#Preliminaries-on-Lagrangian-duals." title="Permalink"></a></h4><p>Given a basic minimization problem with a objective function <span>$f(x)$</span>, inequality constraints <span>$g_i(x)$</span> and equality constraints <span>$h_j(x)$</span>:</p><p class="math-container">\[  \begin{aligned}
  P: \ &amp; \underset{x}{\text{min}}
  &amp; &amp; f(x)  \\
  &amp; \text{s.t.}
  &amp; &amp; g_i(x) \leq 0, \; ∀ i\\
  &amp; &amp; &amp; h_j(x) = 0, \; ∀ j
  \end{aligned}\]</p><p>The Lagrangian is of <span>$P$</span> is <span>$\mathcal{L}(x, α, β) = f(x) + \sum_i α_i g_i(x) + \sum_j β_j h_j(x)$</span> with dual variables <span>$α_i \geq 0$</span> and <span>$β_j \geq 0$</span>.</p><p>Instead of solving the primal problem <span>$P$</span>, one can then solve Wolfe dual problem based on the Lagrangian when the functions <span>$f$</span>, <span>$g_i$</span> and <span>$h_j$</span> are differentiable and convex. Note that Dual problem is now an maximization problem.</p><p class="math-container">\[\begin{aligned}
D: \ &amp; \underset{x, α, β}{\text{max}}
&amp; &amp; f(x) + \sum_i α_i g_i(x) + \sum_j β_j h_j(x)  \\
&amp; \text{s.t.}
&amp; &amp; ∇f(x) + \sum_i α_i ∇ g_i(x) + \sum_j β_j ∇ h_j(x) = 0\\
&amp; &amp; &amp; α_i \geq 0, \; ∀ i \\
&amp; &amp; &amp; β_j \geq 0, \; ∀ j \\
\end{aligned}\]</p><h4 id="SVDD"><a class="docs-heading-anchor" href="#SVDD">SVDD</a><a id="SVDD-1"></a><a class="docs-heading-anchor-permalink" href="#SVDD" title="Permalink"></a></h4><p>Given the primal optimization problem of the SVDD</p><p class="math-container">\[  \begin{aligned}
  P: \ &amp; \underset{R, a, \xi}{\text{min}}
  &amp; &amp; R^2 + C * \sum_i \xi_i  \\
  &amp; \text{s.t.}
  &amp; &amp; \left\Vert \Phi(x_{i}) - a \right\Vert^2 \leq R^2 + \xi_i, \; ∀ i \\
  &amp; &amp; &amp; \xi_i \geq 0, \; ∀ i
  \end{aligned}\]</p><p>we transpose the constraints to <span>$\left\Vert \Phi(x_{i}) - a \right\Vert^2 - R^2 - \xi_i \leq 0$</span> and <span>$-\xi_i \leq 0$</span>. Then the Lagrangian is</p><p class="math-container">\[\begin{aligned}
\mathcal{L}(R, a, α, γ, ξ) &amp;= R^2 + C \sum_i \xi_i + \sum_i α_i (\left\Vert \Phi(x_{i}) - a \right\Vert^2 - R^2 - \xi_i) + \sum_i γ_i (-\xi_i) \\
&amp;= R^2 + C \sum_i \xi_i - \sum_i α_i ( R^2 + \xi_i - \left\Vert \Phi(x_{i}) - a \right\Vert^2) - \sum_i γ_i \xi_i \\
&amp;= R^2 + C \sum_i \xi_i - \sum_i α_i ( R^2 + \xi_i - \left\Vert \Phi(x_{i})\right\Vert^2 - 2\Phi(x_i) a + \left\Vert a\right\Vert^2) - \sum_i γ_i \xi_i
\end{aligned}\]</p><p>with dual variables <span>$α_i \geq 0$</span> and <span>$γ_i \geq 0$</span>.</p><p>Setting the partial derivatives of <span>$\mathcal{L}$</span> to zero gives</p><p class="math-container">\[\begin{aligned}
\dfrac{\partial \mathcal{L}}{\partial R} = 0:\quad&amp;
2R - 2\sum_i α_i R &amp;= 0\\
⇔ \quad&amp; 2R (1 - \sum_i α_i) &amp;= 0\\
⇔ \quad&amp; \sum_i α_i = 1 \quad \text{with } R &gt; 0\\
\dfrac{\partial \mathcal{L}}{\partial a} = 0:\quad&amp;
- \sum \alpha_i(2a - 2 \Phi(x_i)) &amp;= 0\\
⇔ \quad&amp; - a\sum_i \alpha_i  + \sum_i \alpha_i\Phi(x_i) &amp;= 0\\
⇔ \quad&amp; a = \dfrac{\sum_i \alpha_i \Phi(x_i)}{\sum_i \alpha_i} = \sum_i \alpha_i \Phi(x_i)\\
\dfrac{\partial \mathcal{L}}{\partial \xi_i} = 0:\quad&amp;
c - \alpha_i - \gamma_i &amp;= 0&amp;\\
⇔ \quad&amp; \alpha_i = C - \gamma_i&amp;\\
\Rightarrow \quad&amp; 0 \leq \alpha_i \leq C \quad \text{with } \alpha_i, \gamma_i \geq 0\\
\end{aligned}\]</p><p>Substituting back into <span>$\mathcal{L}$</span> gives</p><p class="math-container">\[\begin{aligned}
\mathcal{L}(R, a, α, γ, ξ) &amp;= R^2 + C \sum_i \xi_i - \sum_i α_i \left( R^2 + \xi_i - \left\Vert \Phi(x_{i})\right\Vert^2 - 2\Phi(x_i) a + \left\Vert a\right\Vert^2\right) - \sum_i γ_i \xi_i\\
&amp;= R^2(1 - \underbrace{\sum_i \alpha_i}_{=1}) + \sum_i \xi_i \underbrace{(C - \alpha_i - \gamma_i)}_{=0} + \sum_i \alpha_i (\left\Vert \Phi(x_{i})\right\Vert^2 - 2\Phi(x_i) a + \left\Vert a\right\Vert^2)\\
&amp;= \sum_i \alpha_i (\left\Vert \Phi(x_{i})\right\Vert^2 - 2\Phi(x_i) a + \left\Vert a\right\Vert^2)\\
&amp;= \sum_i \alpha_i \left(\left\Vert \Phi(x_{i})\right\Vert^2 - 2 \Phi(x_i)\sum_j \alpha_j \Phi(x_j) + \left(\sum_j \alpha_j \Phi(x_j)\right)^2 \right)\\
&amp;= \sum_i \alpha_i \left(\left\Vert \Phi(x_{i})\right\Vert^2 - 2 \Phi(x_i)\sum_j \alpha_j \Phi(x_j) + \sum_j \sum_k \alpha_j \alpha_k \Phi(x_j)\Phi(x_k) \right)\\
&amp;=\sum_i \alpha_i\left\Vert \Phi(x_{i})\right\Vert^2 - 2 \sum_i\sum_j \alpha_i \alpha_j \Phi(x_i) \Phi(x_j) + \sum_i\sum_j\sum_k\alpha_i\alpha_j\alpha_k\Phi(x_i)\Phi(x_j)\Phi(x_k)\\
&amp;=\sum_i \alpha_i\left\Vert \Phi(x_{i})\right\Vert^2 - \sum_i\sum_j \alpha_i \alpha_j \Phi(x_i) \Phi(x_j) (2 - \underbrace{\sum_k \alpha_k \Phi(x_k)}_{=1})\\
&amp;=\sum_i \alpha_i\left\Vert \Phi(x_{i})\right\Vert^2 - \sum_i\sum_j \alpha_i \alpha_j \Phi(x_i) \Phi(x_j)\\
&amp;=\sum_i \alpha_i\Phi(x_{i})\Phi(x_i) - \sum_i\sum_j \alpha_i \alpha_j \Phi(x_i) \Phi(x_j)
\end{aligned}\]</p><p>By substituting the inner products with the kernel matrix <span>$K_{i, j} = \Phi(x_i)\Phi(x_j)$</span> and adding the constraints we finally get the dual problem:</p><p class="math-container">\[\begin{aligned}
D: \ &amp; \underset{\alpha}{\text{max}}
&amp; &amp; \sum_{i}\alpha_i K_{i,i} - \sum_i\sum_j\alpha_i\alpha_jK_{i,j}  \\
&amp; \text{s.t.}
&amp; &amp; \sum_i \alpha_i = 1\\
&amp; &amp; &amp; 0 \leq \alpha_i \leq C, \; ∀ i \\
\end{aligned}\]</p><p>The decision function of the SVDD is the distance to the decision boundary; positive for outliers, negative or zero for inliers:</p><p class="math-container">\[f(x_i) = \left\Vert \Phi(x_i) - a \right\Vert^2 - R^2\]</p><p class="math-container">\[R^2\]</p><p>can be calculated with any support vector (SV), i.e., an observation <span>$x_k$</span> with an <span>$\alpha_k$</span> that is <span>$0 &lt; \alpha_k &lt; C$</span>:</p><p class="math-container">\[\begin{aligned}
R^2 &amp;= \left\Vert \Phi(x_k) - a \right\Vert^2\\
&amp; = \Phi(x_k)\Phi(x_k) - 2 \Phi(x_k)\sum_i \alpha_i \Phi(x_i) + \sum_i\sum_j \alpha_i \alpha_j \Phi(x_i) \Phi(x_j)\\
&amp; = K_{k, k} - 2 \sum_i \alpha_i K_{k, i} + \sum_i \sum_j \alpha_i \alpha_j K_{i, j}
\end{aligned}\]</p><p>The final decision function for an arbitrary <span>$x_i$</span> is then:</p><p class="math-container">\[f(x_i) = K_{i, i} - 2 \sum_j \alpha_j K_{i, j} + \underbrace{\sum_j\sum_k \alpha_j \alpha_k K_{j, k}}_{\text{const}} - R^2\]</p><p>The term with the double sum is independent of <span>$x_i$</span> and can be pre-calculcated.</p><h1 id="SVDDneg"><a class="docs-heading-anchor" href="#SVDDneg">SVDDneg</a><a id="SVDDneg-1"></a><a class="docs-heading-anchor-permalink" href="#SVDDneg" title="Permalink"></a></h1><p>The SVDDneg is an extension of the vanillia SVDD and allows to use outlier labels [1]. In the following the key extension of the SVDD are highlighted in red. The outliers have index <span>$l$</span> in the following optimization problem:</p><p class="math-container">\[  \begin{aligned}
  P: \ &amp; \underset{R, a, \xi}{\text{min}}
  &amp; &amp; R^2 + C_1 * \sum_i \xi_i  {\color{red}+ C_2 * \sum_l \xi_l}\\
  &amp; \text{s.t.}
  &amp; &amp; \left\Vert \Phi(x_{i}) - a \right\Vert^2 \leq R^2 + \xi_i, \; ∀ i \\
  &amp; &amp; &amp; {\color{red}\left\Vert \Phi(x_{l}) - a \right\Vert^2 \leq R^2 - \xi_l, \; ∀ l }\\
  &amp; &amp; &amp; \xi_i, \geq 0, \; ∀ i\\
  &amp; &amp; &amp; {\color{red}\xi_l, \geq 0, \; ∀ l}
  \end{aligned}\]</p><p>with radius <span>$R$</span> and center of the hypersphere <span>$a$</span>, a slack variable <span>$\xi$</span>, and a mapping into an implicit feature space <span>$\Phi$</span>.</p><p>The dual SVDDneg problem with inlier/unlabeled indices <span>$i$</span>, <span>$j$</span> and outlier indices <span>$l$</span>, <span>$m$</span> of the Lagrangian is:</p><p class="math-container">\[\begin{aligned}
D: \ &amp; \underset{\alpha}{\text{max}}
&amp; &amp; \sum_{i}\alpha_i K_{i,i} - \sum_l K_{l, l} - \sum_i \sum_j \alpha_i \alpha_j K_{i, j}\\
&amp; &amp; &amp;+ 2\sum_l\sum_j \alpha_l \alpha_j K_{l, j} - \sum_l\sum_m\alpha_l\alpha_mK_{l,m}  \\
&amp; \text{s.t.}
&amp; &amp; \sum_i \alpha_i = 1 \\
&amp; &amp; &amp; 0 \leq \alpha_i \leq C_1, \; ∀ i \\
&amp; &amp; &amp; 0 \leq \alpha_i \leq C_2, \; ∀ l \\
\end{aligned}\]</p><p>where <span>$\alpha$</span> are the Lagrange multipliers, and <span>$K_{i,j} = \langle {\Phi(x_i),\Phi{x_j}} \rangle$</span> the inner product in the implicit feature space.</p><p>[1] Tax, David MJ, and Robert PW Duin. &quot;Support vector data description.&quot; Machine learning 54.1 (2004): 45-66. [2] Chang, Wei-Cheng, Ching-Pei Lee, and Chih-Jen Lin. &quot;A revisit to support vector data description.&quot; Dept. Comput. Sci., Nat. Taiwan Univ., Taipei, Taiwan, Tech. Rep (2013).</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../start/">« Getting Started</a><a class="docs-footer-nextpage" href="../smo/">SMO »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.11 on <span class="colophon-date" title="Sunday 16 January 2022 21:57">Sunday 16 January 2022</span>. Using Julia version 1.7.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
