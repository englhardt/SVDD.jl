<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>SVDD · SVDD Documentation</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>SVDD Documentation</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../start/">Getting Started</a></li><li class="current"><a class="toctext" href>SVDD</a><ul class="internal"><li><a class="toctext" href="#SVDD-Overview-1">SVDD Overview</a></li><li><a class="toctext" href="#Derivation-of-the-Dual-Problem-1">Derivation of the Dual Problem</a></li><li class="toplevel"><a class="toctext" href="#SVDDneg-1">SVDDneg</a></li></ul></li><li><span class="toctext">Custom Solvers</span><ul><li><a class="toctext" href="../smo/">SMO</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>SVDD</a></li></ul><a class="edit-page" href="https://github.com/englhardt/SVDD.jl/blob/master/docs/src/svdd.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>SVDD</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="svdd_doc-1" href="#svdd_doc-1">SVDD</a></h1><p>Support Vector Data Description (SVDD) is a one-class classifier and was first presented in [1].</p><p>This documentation first presents the optimization problem and then guides the guides the reader to the dual problem</p><h2><a class="nav-anchor" id="SVDD-Overview-1" href="#SVDD-Overview-1">SVDD Overview</a></h2><p>SVDD is an optimization problem of the following form.</p><div>\[  \begin{aligned}
  P: \ &amp; \underset{R, a, \xi}{\text{min}}
  &amp; &amp; R^2 + C * \sum_i \xi_i  \\
  &amp; \text{s.t.}
  &amp; &amp; \left\Vert \Phi(x_{i}) - a \right\Vert^2 \leq R^2 + \xi_i, \; ∀ i \\
  &amp; &amp; &amp; \xi_i \geq 0, \; ∀ i
  \end{aligned}\]</div><p>with radius <span>$R$</span> and center of the hypersphere <span>$a$</span>, a slack variable <span>$\xi$</span>, and a mapping into an implicit feature space <span>$\Phi$</span>.</p><p>The dual Problem of the Lagrangian is:</p><div>\[\begin{aligned}
D: \ &amp; \underset{\alpha}{\text{max}}
&amp; &amp; \sum_{i}\alpha_i K_{i,i} - \sum_i\sum_j\alpha_i\alpha_jK_{i,j}  \\
&amp; \text{s.t.}
&amp; &amp; \sum_i \alpha_i = 1 \\
&amp; &amp; &amp; 0 \leq \alpha_i \leq C, \; ∀ i \\
\end{aligned}\]</div><p>where <span>$\alpha$</span> are the Lagrange multipliers, and <span>$K_{i,j} = \langle {\Phi(x_i),\Phi{x_j}} \rangle$</span> the inner product in the implicit feature space. Solving the Lagrangian gives an optimal <span>$α$</span>. The following rules are valid for the optimal <span>$α$</span>:</p><div>\[\begin{aligned}
\text{(I)} \quad &amp; \left\Vert \Phi(x_{i}) - a \right\Vert^2 \lt R^2 &amp; ⇒ &amp;\quad α_i = 0\\
\text{(II)} \quad &amp; \left\Vert \Phi(x_{i}) - a \right\Vert^2 = R^2 &amp; ⇒ &amp;\quad 0 &lt; α_i &lt; C\\
\text{(III)} \quad &amp; \left\Vert \Phi(x_{i}) - a \right\Vert^2 \gt R^2 &amp; ⇒ &amp;\quad α_i = C
\end{aligned}\]</div><p>An observation is an inlier if (I) or (II) holds and an outlier if (III) holds. Additionally, an observation is called a support vector (SV) if (II) holds. One can calculate the radius of the hypersphere from any SV as we will explain later.</p><h2><a class="nav-anchor" id="Derivation-of-the-Dual-Problem-1" href="#Derivation-of-the-Dual-Problem-1">Derivation of the Dual Problem</a></h2><h4><a class="nav-anchor" id="Preliminaries-on-Lagrangian-duals.-1" href="#Preliminaries-on-Lagrangian-duals.-1">Preliminaries on Lagrangian duals.</a></h4><p>Given a basic minimization problem with a objective function <span>$f(x)$</span>, inequality constraints <span>$g_i(x)$</span> and equality constraints <span>$h_j(x)$</span>:</p><div>\[  \begin{aligned}
  P: \ &amp; \underset{x}{\text{min}}
  &amp; &amp; f(x)  \\
  &amp; \text{s.t.}
  &amp; &amp; g_i(x) \leq 0, \; ∀ i\\
  &amp; &amp; &amp; h_j(x) = 0, \; ∀ j
  \end{aligned}\]</div><p>The Lagrangian is of <span>$P$</span> is <span>$\mathcal{L}(x, α, β) = f(x) + \sum_i α_i g_i(x) + \sum_j β_j h_j(x)$</span> with dual variables <span>$α_i \geq 0$</span> and <span>$β_j \geq 0$</span>.</p><p>Instead of solving the primal problem <span>$P$</span>, one can then solve Wolfe dual problem based on the Lagrangian when the functions <span>$f$</span>, <span>$g_i$</span> and <span>$h_j$</span> are differentiable and convex. Note that Dual problem is now an maximization problem.</p><div>\[\begin{aligned}
D: \ &amp; \underset{x, α, β}{\text{max}}
&amp; &amp; f(x) + \sum_i α_i g_i(x) + \sum_j β_j h_j(x)  \\
&amp; \text{s.t.}
&amp; &amp; ∇f(x) + \sum_i α_i ∇ g_i(x) + \sum_j β_j ∇ h_j(x) = 0\\
&amp; &amp; &amp; α_i \geq 0, \; ∀ i \\
&amp; &amp; &amp; β_j \geq 0, \; ∀ j \\
\end{aligned}\]</div><h4><a class="nav-anchor" id="SVDD-1" href="#SVDD-1">SVDD</a></h4><p>Given the primal optimization problem of the SVDD</p><div>\[  \begin{aligned}
  P: \ &amp; \underset{R, a, \xi}{\text{min}}
  &amp; &amp; R^2 + C * \sum_i \xi_i  \\
  &amp; \text{s.t.}
  &amp; &amp; \left\Vert \Phi(x_{i}) - a \right\Vert^2 \leq R^2 + \xi_i, \; ∀ i \\
  &amp; &amp; &amp; \xi_i \geq 0, \; ∀ i
  \end{aligned}\]</div><p>we transpose the constraints to <span>$\left\Vert \Phi(x_{i}) - a \right\Vert^2 - R^2 - \xi_i \leq 0$</span> and <span>$-\xi_i \leq 0$</span>. Then the Lagrangian is</p><div>\[\begin{aligned}
\mathcal{L}(R, a, α, γ, ξ) &amp;= R^2 + C \sum_i \xi_i + \sum_i α_i (\left\Vert \Phi(x_{i}) - a \right\Vert^2 - R^2 - \xi_i) + \sum_i γ_i (-\xi_i) \\
&amp;= R^2 + C \sum_i \xi_i - \sum_i α_i ( R^2 + \xi_i - \left\Vert \Phi(x_{i}) - a \right\Vert^2) - \sum_i γ_i \xi_i \\
&amp;= R^2 + C \sum_i \xi_i - \sum_i α_i ( R^2 + \xi_i - \left\Vert \Phi(x_{i})\right\Vert^2 - 2\Phi(x_i) a + \left\Vert a\right\Vert^2) - \sum_i γ_i \xi_i
\end{aligned}\]</div><p>with dual variables <span>$α_i \geq 0$</span> and <span>$γ_i \geq 0$</span>.</p><p>Setting the partial derivatives of <span>$\mathcal{L}$</span> to zero gives</p><div>\[\begin{aligned}
\dfrac{\partial \mathcal{L}}{\partial R} = 0:\quad&amp;
2R - 2\sum_i α_i R &amp;= 0\\
⇔ \quad&amp; 2R (1 - \sum_i α_i) &amp;= 0\\
⇔ \quad&amp; \sum_i α_i = 1 \quad \text{with } R &gt; 0\\
\dfrac{\partial \mathcal{L}}{\partial a} = 0:\quad&amp;
- \sum \alpha_i(2a - 2 \Phi(x_i)) &amp;= 0\\
⇔ \quad&amp; - a\sum_i \alpha_i  + \sum_i \alpha_i\Phi(x_i) &amp;= 0\\
⇔ \quad&amp; a = \dfrac{\sum_i \alpha_i \Phi(x_i)}{\sum_i \alpha_i} = \sum_i \alpha_i \Phi(x_i)\\
\dfrac{\partial \mathcal{L}}{\partial \xi_i} = 0:\quad&amp;
c - \alpha_i - \gamma_i &amp;= 0&amp;\\
⇔ \quad&amp; \alpha_i = C - \gamma_i&amp;\\
\Rightarrow \quad&amp; 0 \leq \alpha_i \leq C \quad \text{with } \alpha_i, \gamma_i \geq 0\\
\end{aligned}\]</div><p>Substituting back into <span>$\mathcal{L}$</span> gives</p><div>\[\begin{aligned}
\mathcal{L}(R, a, α, γ, ξ) &amp;= R^2 + C \sum_i \xi_i - \sum_i α_i \left( R^2 + \xi_i - \left\Vert \Phi(x_{i})\right\Vert^2 - 2\Phi(x_i) a + \left\Vert a\right\Vert^2\right) - \sum_i γ_i \xi_i\\
&amp;= R^2(1 - \underbrace{\sum_i \alpha_i}_{=1}) + \sum_i \xi_i \underbrace{(C - \alpha_i - \gamma_i)}_{=0} + \sum_i \alpha_i (\left\Vert \Phi(x_{i})\right\Vert^2 - 2\Phi(x_i) a + \left\Vert a\right\Vert^2)\\
&amp;= \sum_i \alpha_i (\left\Vert \Phi(x_{i})\right\Vert^2 - 2\Phi(x_i) a + \left\Vert a\right\Vert^2)\\
&amp;= \sum_i \alpha_i \left(\left\Vert \Phi(x_{i})\right\Vert^2 - 2 \Phi(x_i)\sum_j \alpha_j \Phi(x_j) + \left(\sum_j \alpha_j \Phi(x_j)\right)^2 \right)\\
&amp;= \sum_i \alpha_i \left(\left\Vert \Phi(x_{i})\right\Vert^2 - 2 \Phi(x_i)\sum_j \alpha_j \Phi(x_j) + \sum_j \sum_k \alpha_j \alpha_k \Phi(x_j)\Phi(x_k) \right)\\
&amp;=\sum_i \alpha_i\left\Vert \Phi(x_{i})\right\Vert^2 - 2 \sum_i\sum_j \alpha_i \alpha_j \Phi(x_i) \Phi(x_j) + \sum_i\sum_j\sum_k\alpha_i\alpha_j\alpha_k\Phi(x_i)\Phi(x_j)\Phi(x_k)\\
&amp;=\sum_i \alpha_i\left\Vert \Phi(x_{i})\right\Vert^2 - \sum_i\sum_j \alpha_i \alpha_j \Phi(x_i) \Phi(x_j) (2 - \underbrace{\sum_k \alpha_k \Phi(x_k)}_{=1})\\
&amp;=\sum_i \alpha_i\left\Vert \Phi(x_{i})\right\Vert^2 - \sum_i\sum_j \alpha_i \alpha_j \Phi(x_i) \Phi(x_j)\\
&amp;=\sum_i \alpha_i\Phi(x_{i})\Phi(x_i) - \sum_i\sum_j \alpha_i \alpha_j \Phi(x_i) \Phi(x_j)
\end{aligned}\]</div><p>By substituting the inner products with the kernel matrix <span>$K_{i, j} = \Phi(x_i)\Phi(x_j)$</span> and adding the constraints we finally get the dual problem:</p><div>\[\begin{aligned}
D: \ &amp; \underset{\alpha}{\text{max}}
&amp; &amp; \sum_{i}\alpha_i K_{i,i} - \sum_i\sum_j\alpha_i\alpha_jK_{i,j}  \\
&amp; \text{s.t.}
&amp; &amp; \sum_i \alpha_i = 1\\
&amp; &amp; &amp; 0 \leq \alpha_i \leq C, \; ∀ i \\
\end{aligned}\]</div><p>The decision function of the SVDD is the distance to the decision boundary; positive for outliers, negative or zero for inliers:</p><div>\[f(x_i) = \left\Vert \Phi(x_i) - a \right\Vert^2 - R^2\]</div><div>\[R^2\]</div><p>can be calculated with any support vector (SV), i.e., an observation <span>$x_k$</span> with an <span>$\alpha_k$</span> that is <span>$0 &lt; \alpha_k &lt; C$</span>:</p><div>\[\begin{aligned}
R^2 &amp;= \left\Vert \Phi(x_k) - a \right\Vert^2\\
&amp; = \Phi(x_k)\Phi(x_k) - 2 \Phi(x_k)\sum_i \alpha_i \Phi(x_i) + \sum_i\sum_j \alpha_i \alpha_j \Phi(x_i) \Phi(x_j)\\
&amp; = K_{k, k} - 2 \sum_i \alpha_i K_{k, i} + \sum_i \sum_j \alpha_i \alpha_j K_{i, j}
\end{aligned}\]</div><p>The final decision function for an arbitrary <span>$x_i$</span> is then:</p><div>\[f(x_i) = K_{i, i} - 2 \sum_j \alpha_j K_{i, j} + \underbrace{\sum_j\sum_k \alpha_j \alpha_k K_{j, k}}_{\text{const}} - R^2\]</div><p>The term with the double sum is independent of <span>$x_i$</span> and can be pre-calculcated.</p><h1><a class="nav-anchor" id="SVDDneg-1" href="#SVDDneg-1">SVDDneg</a></h1><p>The SVDDneg is an extension of the vanillia SVDD and allows to use outlier labels [1]. In the following the key extension of the SVDD are highlighted in red. The outliers have index <span>$l$</span> in the following optimization problem:</p><div>\[  \begin{aligned}
  P: \ &amp; \underset{R, a, \xi}{\text{min}}
  &amp; &amp; R^2 + C_1 * \sum_i \xi_i  {\color{red}+ C_2 * \sum_l \xi_l}\\
  &amp; \text{s.t.}
  &amp; &amp; \left\Vert \Phi(x_{i}) - a \right\Vert^2 \leq R^2 + \xi_i, \; ∀ i \\
  &amp; &amp; &amp; {\color{red}\left\Vert \Phi(x_{l}) - a \right\Vert^2 \leq R^2 - \xi_l, \; ∀ l }\\
  &amp; &amp; &amp; \xi_i, \geq 0, \; ∀ i\\
  &amp; &amp; &amp; {\color{red}\xi_l, \geq 0, \; ∀ l}
  \end{aligned}\]</div><p>with radius <span>$R$</span> and center of the hypersphere <span>$a$</span>, a slack variable <span>$\xi$</span>, and a mapping into an implicit feature space <span>$\Phi$</span>.</p><p>The dual SVDDneg problem with inlier/unlabeled indices <span>$i$</span>, <span>$j$</span> and outlier indices <span>$l$</span>, <span>$m$</span> of the Lagrangian is:</p><div>\[\begin{aligned}
D: \ &amp; \underset{\alpha}{\text{max}}
&amp; &amp; \sum_{i}\alpha_i K_{i,i} - \sum_l K_{l, l} - \sum_i \sum_j \alpha_i \alpha_j K_{i, j}\\
&amp; &amp; &amp;+ 2\sum_l\sum_j \alpha_l \alpha_j K_{l, j} - \sum_l\sum_m\alpha_l\alpha_mK_{l,m}  \\
&amp; \text{s.t.}
&amp; &amp; \sum_i \alpha_i = 1 \\
&amp; &amp; &amp; 0 \leq \alpha_i \leq C_1, \; ∀ i \\
&amp; &amp; &amp; 0 \leq \alpha_i \leq C_2, \; ∀ l \\
\end{aligned}\]</div><p>where <span>$\alpha$</span> are the Lagrange multipliers, and <span>$K_{i,j} = \langle {\Phi(x_i),\Phi{x_j}} \rangle$</span> the inner product in the implicit feature space.</p><p>[1] Tax, David MJ, and Robert PW Duin. &quot;Support vector data description.&quot; Machine learning 54.1 (2004): 45-66. [2] Chang, Wei-Cheng, Ching-Pei Lee, and Chih-Jen Lin. &quot;A revisit to support vector data description.&quot; Dept. Comput. Sci., Nat. Taiwan Univ., Taipei, Taiwan, Tech. Rep (2013).</p><footer><hr/><a class="previous" href="../start/"><span class="direction">Previous</span><span class="title">Getting Started</span></a><a class="next" href="../smo/"><span class="direction">Next</span><span class="title">SMO</span></a></footer></article></body></html>
